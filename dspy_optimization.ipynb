{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nazara/Data/UCU/FINETUNE_GENAI/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dspy\n",
    "\n",
    "\n",
    "with open('gen-ai-ucu-2024-task-3/zno.train.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "all_questions = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_questions.append(result)\n",
    "\n",
    "all_examples = []\n",
    "for sample in all_questions:\n",
    "    example = dspy.Example(\n",
    "        question=sample['question'],\n",
    "        options=sample['answers'],\n",
    "        correct_answer = sample['correct_answers'][0]\n",
    "    ).with_inputs(\"question\", \"options\")\n",
    "    all_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load for creating submission\n",
    "with open('gen-ai-ucu-2024-task-3/zno.test.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "sumbission_all_questions = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    sumbission_all_questions.append(result)\n",
    "\n",
    "sumbission_all_examples = []\n",
    "for sample in sumbission_all_questions:\n",
    "    example = dspy.Example(\n",
    "        question=sample['question'],\n",
    "        options=sample['answers'],\n",
    "    ).with_inputs(\"question\", \"options\")\n",
    "    sumbission_all_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = all_examples[int(len(all_questions)*0.2):], all_examples[:int(len(all_questions)*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U need to launch vllm \n",
    "\n",
    "# vllm serve unsloth/Meta-Llama-3.1-8B-bnb-4bit --dtype float16 --gpu-memory-utilization 0.8 --max-model-len 13500 --quantization bitsandbytes --load-format bitsandbytes --served-model-name meta-llama/Llama-3.1-8B --trust-remote-code --chat-template llama_3.1_template.jinja\n",
    "\n",
    "#vllm serve unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit --dtype float16 --gpu-memory-utilization 0.8 --max-model-len 13500 --quantization bitsandbytes --load-format bitsandbytes --served-model-name meta-llama/Llama-3.1-8B-Instruct --trust-remote-code\n",
    "\n",
    "# vllm serve unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit \\\n",
    "#  --dtype float16 --gpu-memory-utilization 0.8 --max-model-len 13500 --quantization bitsandbytes \\\n",
    "#  --load-format bitsandbytes --served-model-name meta-llama/Llama-3.1-8B-Instruct --trust-remote-code \\\n",
    "#  --enable-lora \\\n",
    "#  --lora-modules mylora120steps=./lora_adapter_llama8b_120\n",
    "\n",
    "\n",
    "# vllm serve unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit \\\n",
    "#  --dtype float16 --gpu-memory-utilization 0.8 --max-model-len 13500 --quantization bitsandbytes \\\n",
    "#  --load-format bitsandbytes --served-model-name meta-llama/Llama-3.1-8B-Instruct --trust-remote-code \\\n",
    "#  --enable-lora \\\n",
    "#  --lora-modules mylora3epochs=./lora_adapter_llama8b_30epochs\n",
    "\n",
    "# vllm serve meta-llama/Llama-2-7b-hf \\\n",
    "# vllm serve unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit  --dtype float16 --gpu-memory-utilization 0.8 --max-model-len 13500 --quantization bitsandbytes  --load-format bitsandbytes --served-model-name meta-llama/Llama-3.1-8B-Instruct --trust-remote-code  --enable-lora  --lora-modules raglora_4=./lorai_adapter_llama8b_5epochs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('openai/lora_increased',api_base=\"http://localhost:8000/v1\",  # ensure this points to your port\n",
    "             api_key=\"local\", model_type='chat')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Hello new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Categorize(dspy.Signature):\n",
    "    \"\"\"Solve exam problem.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    options: list[dict[str,str]] = dspy.InputField()\n",
    "    correct_marker: str = dspy.OutputField()\n",
    "\n",
    "classify = dspy.Predict(Categorize, False,cache=False)\n",
    "\n",
    "#loading optimized prompt\n",
    "classify.load(\"BootstrapFewShotWithRandomSearch_program_15_candidates_llama.json\")\n",
    "\n",
    "# Here is how we call this module\n",
    "classification = classify(question = all_examples[-1].question, options = all_examples[-1].options, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'А'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.correct_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [04:56<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "classifications = []\n",
    "correct_answers = []\n",
    "for sample in tqdm(test_set):\n",
    "    classification = classify(question = sample.question, options = sample.options)\n",
    "    classifications.append(classification.correct_marker)\n",
    "    correct_answers.append(sample.correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_answers = ['А', 'Б', 'В', 'Г', 'Д', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def post_process_sample(prediction:str) -> str:\n",
    "    found_answer = 'А'  # Default value\n",
    "\n",
    "    for allowed_answer in allowed_answers:\n",
    "        if allowed_answer in prediction:\n",
    "            found_answer = allowed_answer\n",
    "            # print(f\"answer {found_answer} for {classification}\")\n",
    "            break\n",
    "    return found_answer\n",
    "\n",
    "post_processed = []\n",
    "\n",
    "for classification in classifications:\n",
    "    found_answer = post_process_sample(classification)\n",
    "    post_processed.append(found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Simple evaluation of classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions (list): Model's predicted answers\n",
    "    ground_truth (list): Correct answers\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    \n",
    "    # Get unique options\n",
    "    unique_options = sorted(list(set(predictions + ground_truth)))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(ground_truth, predictions, labels=unique_options)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(\"Options:\", unique_options)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4346\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 0  0  0  0  1  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 62 16  9 14 11]\n",
      " [ 0  0 42 72 20 12 19]\n",
      " [ 0  0 34 20 52 17 20]\n",
      " [ 0  0 38 16 17 51  9]\n",
      " [ 0  0 12  7  4  7 29]]\n"
     ]
    }
   ],
   "source": [
    "# after training 5 epochs increased lora\n",
    "evaluate_model(classifications,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4346\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 0  0  0  0  1  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 62 16  9 14 11]\n",
      " [ 0  0 42 72 20 12 19]\n",
      " [ 0  0 34 20 52 17 20]\n",
      " [ 0  0 38 16 17 51  9]\n",
      " [ 0  0 12  7  4  7 29]]\n"
     ]
    }
   ],
   "source": [
    "# after training 5 epochs increased lora\n",
    "evaluate_model(post_processed,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4003\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  1  0  0]\n",
      " [ 0  0 40 16 29 15 12]\n",
      " [ 0  0 18 56 36 27 28]\n",
      " [ 0  0 16 22 64 17 24]\n",
      " [ 0  0 12 12 33 52 22]\n",
      " [ 0  0  5  6 12  3 33]]\n"
     ]
    }
   ],
   "source": [
    "# after training 3.25 epochs\n",
    "evaluate_model(classifications,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4003\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  1  0  0]\n",
      " [ 0  0 40 16 29 15 12]\n",
      " [ 0  0 18 56 36 27 28]\n",
      " [ 0  0 16 22 64 17 24]\n",
      " [ 0  0 12 12 33 52 22]\n",
      " [ 0  0  5  6 12  3 33]]\n"
     ]
    }
   ],
   "source": [
    "# after training 3.25 epochs\n",
    "evaluate_model(post_processed,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3121\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0]\n",
      " [ 0  0 82 13  6  9  2]\n",
      " [ 0  0 88 48  8 15  6]\n",
      " [ 0  0 79 21 23 11  9]\n",
      " [ 0  0 68 17 10 29  7]\n",
      " [ 0  0 38  9  0  4  8]]\n"
     ]
    }
   ],
   "source": [
    "# after training 120 steps\n",
    "evaluate_model(classifications,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3121\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0]\n",
      " [ 0  0 82 13  6  9  2]\n",
      " [ 0  0 88 48  8 15  6]\n",
      " [ 0  0 79 21 23 11  9]\n",
      " [ 0  0 68 17 10 29  7]\n",
      " [ 0  0 38  9  0  4  8]]\n"
     ]
    }
   ],
   "source": [
    "# after training 120 steps\n",
    "evaluate_model(post_processed,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2565\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['\"А\"', '\"Б\"', '\"В\"', '(1)Уміння радіти чужим успіхам мірило шляхетності. (2)Якщо людину підтримати похвалити підкреслити щось хороше в ній вона намагається дорівнювати уявленню яке про неї склалося. (3)Ось чому так важливо з дитинства культивувати в людині почуття власної гідності відчуття певної „собівартості” аби згодом не склався комплекс меншовартості. (4)Недарма кажуть Якщо казати людині постійно що вона свиня то згодом вона зарохкає. (5)Не ввічливо кидати компліменти малознайомим або незнайомим людям. (6)Краще зачекати познайомившись ближче чи навіть заприятелювавши з ними. (7)Але пам’ятаймо що слова похвали потрібні кожному.', '1', '4', '[]', 'А', 'А, Б, В, Г, Д', 'А-1, Б-2, В-3, Г-4, Д-5', 'АВГД', 'Б', 'В', 'Василеві Стусу', 'Г', 'Д']\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  1  0  0  0  0  0 13  0  0  1 16 67  0 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  1  0  0  0  0 15  0  1  0 36 89  0 14  5]\n",
      " [ 1  0  2  0  0  0  0 13  0  0  0 24 85  1 15  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  1  0  0 23 77  0 20  1]\n",
      " [ 0  0  0  0  0  0  0  5  0  0  1  6 39  0  5  3]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(classifications,correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2712\n",
      "\n",
      "Confusion Matrix:\n",
      "Options: ['1', '4', 'А', 'Б', 'В', 'Г', 'Д']\n",
      "[[ 0  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0 16 17 67 11  1]\n",
      " [ 0  0 17 39 90 14  5]\n",
      " [ 0  0 14 24 88 15  2]\n",
      " [ 0  0 10 23 77 20  1]\n",
      " [ 0  0  6  6 39  5  3]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(post_processed,correct_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPY OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_category(example, prediction, trace=None):\n",
    "    return prediction.category == example.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(gold, pred, trace=None):\n",
    "    post_processed = post_process_sample(pred.correct_marker)\n",
    "    return post_processed == gold.correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 166.00 / 612 (27.1%): 100%|██████████| 612/612 [00:00<00:00, 1536.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:00:23 INFO dspy.evaluate.evaluate: Average Metric: 166 / 612 (27.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "evaluate = Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=accuracy_metric,\n",
    "    num_threads=4,\n",
    "    display_progress=True,\n",
    "    display_table=0,\n",
    "    max_errors=1,\n",
    "    failure_score = 0\n",
    ")\n",
    "\n",
    "# zero-shot evaluation on examples\n",
    "score,outputs = evaluate(dspy.Predict(Categorize, False),return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_train_set = train_set[:451]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smaller_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import (\n",
    "    BootstrapFewShot,\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    "    KNNFewShot,\n",
    "    MIPROv2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 1 traces per predictor.\n",
      "Will attempt to bootstrap 15 candidate sets.\n",
      "Average Metric: 166.00 / 612 (27.1%): 100%|██████████| 612/612 [00:00<00:00, 1195.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:04:18 INFO dspy.evaluate.evaluate: Average Metric: 166 / 612 (27.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 27.12 for seed -3\n",
      "Scores so far: [27.12]\n",
      "Best score so far: 27.12\n",
      "Average Metric: 166.00 / 612 (27.1%): 100%|██████████| 612/612 [00:00<00:00, 1796.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:04:19 INFO dspy.evaluate.evaluate: Average Metric: 166 / 612 (27.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12]\n",
      "Best score so far: 27.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/451 [00:00<00:00, 1594.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 189.00 / 612 (30.9%): 100%|██████████| 612/612 [04:37<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:08:56 INFO dspy.evaluate.evaluate: Average Metric: 189 / 612 (30.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 30.88 for seed -1\n",
      "Scores so far: [27.12, 27.12, 30.88]\n",
      "Best score so far: 30.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<02:51,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 189.00 / 612 (30.9%): 100%|██████████| 612/612 [04:21<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:13:18 INFO dspy.evaluate.evaluate: Average Metric: 189 / 612 (30.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88]\n",
      "Best score so far: 30.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6/451 [00:02<03:34,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Average Metric: 193.00 / 612 (31.5%): 100%|██████████| 612/612 [04:30<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:17:52 INFO dspy.evaluate.evaluate: Average Metric: 193 / 612 (31.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 31.54 for seed 1\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54]\n",
      "Best score so far: 31.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/451 [00:01<02:43,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Average Metric: 193.00 / 612 (31.5%): 100%|██████████| 612/612 [04:15<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:22:09 INFO dspy.evaluate.evaluate: Average Metric: 193 / 612 (31.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54]\n",
      "Best score so far: 31.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/451 [00:00<02:44,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 186.00 / 612 (30.4%): 100%|██████████| 612/612 [04:14<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:26:24 INFO dspy.evaluate.evaluate: Average Metric: 186 / 612 (30.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39]\n",
      "Best score so far: 31.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<03:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 199.00 / 612 (32.5%): 100%|██████████| 612/612 [04:08<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:30:33 INFO dspy.evaluate.evaluate: Average Metric: 199 / 612 (32.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 32.52 for seed 4\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52]\n",
      "Best score so far: 32.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/451 [00:02<09:35,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 203.00 / 612 (33.2%): 100%|██████████| 612/612 [04:28<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:35:04 INFO dspy.evaluate.evaluate: Average Metric: 203 / 612 (33.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 33.17 for seed 5\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/451 [00:00<02:48,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 200.00 / 612 (32.7%): 100%|██████████| 612/612 [04:55<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:39:59 INFO dspy.evaluate.evaluate: Average Metric: 200 / 612 (32.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/451 [00:01<02:40,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Average Metric: 192.00 / 612 (31.4%): 100%|██████████| 612/612 [04:25<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:44:26 INFO dspy.evaluate.evaluate: Average Metric: 192 / 612 (31.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/451 [00:04<03:17,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 11 examples for up to 1 rounds, amounting to 11 attempts.\n",
      "Average Metric: 195.00 / 612 (31.9%): 100%|██████████| 612/612 [04:20<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:48:52 INFO dspy.evaluate.evaluate: Average Metric: 195 / 612 (31.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<02:51,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 188.00 / 612 (30.7%): 100%|██████████| 612/612 [04:16<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:53:08 INFO dspy.evaluate.evaluate: Average Metric: 188 / 612 (30.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<02:39,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 185.00 / 612 (30.2%): 100%|██████████| 612/612 [04:19<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 19:57:28 INFO dspy.evaluate.evaluate: Average Metric: 185 / 612 (30.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72, 30.23]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<02:46,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 186.00 / 612 (30.4%): 100%|██████████| 612/612 [04:18<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 20:01:47 INFO dspy.evaluate.evaluate: Average Metric: 186 / 612 (30.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72, 30.23, 30.39]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/451 [00:01<02:48,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Average Metric: 182.00 / 612 (29.7%): 100%|██████████| 612/612 [04:31<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 20:06:20 INFO dspy.evaluate.evaluate: Average Metric: 182 / 612 (29.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72, 30.23, 30.39, 29.74]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<00:00, 1343.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 199.00 / 612 (32.5%): 100%|██████████| 612/612 [00:00<00:00, 1081.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 20:06:20 INFO dspy.evaluate.evaluate: Average Metric: 199 / 612 (32.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72, 30.23, 30.39, 29.74, 32.52]\n",
      "Best score so far: 33.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:00<02:41,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 195.00 / 612 (31.9%): 100%|██████████| 612/612 [04:19<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 20:10:40 INFO dspy.evaluate.evaluate: Average Metric: 195 / 612 (31.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [27.12, 27.12, 30.88, 30.88, 31.54, 31.54, 30.39, 32.52, 33.17, 32.68, 31.37, 31.86, 30.72, 30.23, 30.39, 29.74, 32.52, 31.86]\n",
      "Best score so far: 33.17\n",
      "18 candidate programs found.\n",
      "Average Metric: 203.00 / 612 (33.2%): 100%|██████████| 612/612 [00:00<00:00, 827.84it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/11 20:10:41 INFO dspy.evaluate.evaluate: Average Metric: 203 / 612 (33.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BootstrapFewShotWithRandomSearch score : 33.17\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"max_bootstrapped_demos\": 1,\n",
    "    \"max_labeled_demos\": 1,\n",
    "    \"num_candidate_programs\": 15,\n",
    "    \"num_threads\": 4,\n",
    "}\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(metric=accuracy_metric, **config)\n",
    "optimized_program = optimizer.compile(dspy.Predict(Categorize, False), trainset=smaller_train_set,valset=test_set)\n",
    "score = evaluate(optimized_program)\n",
    "\n",
    "try:\n",
    "    optimized_program.save(\"BootstrapFewShotWithRandomSearch_program_15_candidates_llama.json\")\n",
    "except:\n",
    "    print(\"Error saving\")\n",
    "# score for BootstrapFewShotWithRandomSearch evaluation with `ner_metric` function\n",
    "print(f\"BootstrapFewShotWithRandomSearch score : {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751/751 [05:41<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "classifications = []\n",
    "for sample in tqdm(sumbission_all_examples):\n",
    "    classification = classify(question = sample.question, options = sample.options)\n",
    "    classifications.append(classification.correct_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({'id':list(range(751)), \"correct_answers\":classifications}).to_csv(\"submission_1.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
